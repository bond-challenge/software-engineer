--------------------------------------------------------------------------------
Requirements
--------------------------------------------------------------------------------

- Handle large write volume: Billions of write events per day.
- Handle large read/query volume: Millions of merchants wish to gain insight into
  their business. Read/Query patterns are time-series related metrics.
- Provide metrics to customers with at most one hour delay.
- Run with minimum downtime.
- Have the ability to reprocess historical data in case of bugs in the processing
  logic.

--------------------------------------------------------------------------------
Event details
--------------------------------------------------------------------------------
Both POS system and Online system would create an event which consist of following
fields:

    field           bytes
- customer_id         4
- store_id            4
- order_id            8
- product{[
    {
    product_id,       4
    product_quantity  4
    },
    {
    product_id,       4
    product_quantity  4
    }
  ]}
- transaction_amount  8
- transaction_id      8
- order_ts            10
- transaction_ts      10
- delivery_id         4
- delivery_type       4

Assuming on an average a customer order's 4 different product types, total bytes
for product object ~ 32 bytes

order_id -> combination key(store_id + epoch ts)
transaction_id -> combination key (customer_id + epoch ts)

Each event would consist 92 bytes. For simplification let us assume 100 bytes.

--------------------------------------------------------------------------------
Capacity estimation
--------------------------------------------------------------------------------
Let us assume 1 billion write events are generated on daily basis.

Assuming 80% of write events are generated by online system and 20% by POS system.
Also the pizza stores are operated 12 hours a day.

- Online (800 million events/daily)
  - total online data => 800 million * 100 bytes = 80 GB/daily
  - On a hourly basis = 6.67GB

- POS system (200 million events/daily)
  - With 15 minutes interval, there will be 48 batches ~ assuming 50 batches for simple calculations
  - Each batch would roughly consist of 4 million events
  - Each batch size = 400 MB
  - As there are 2000 stores, we would be getting around 2000 files in a batch of 200KB size.
  - On a daily basis, data received = 19.2GB ~ 20 GB
  - On a hourly basis, data received = 1.6GB

- Total data received
  - Daily = 100GB
  - Yearly = 3.65 TB
  - Designing for 3 Years growth ~ 12 TB of space is required.
--------------------------------------------------------------------------------
High level system design
--------------------------------------------------------------------------------

- Pizza Online Store
  Customers who are placing orders online, would visit the online website, select there
  store, products and place an online. This pizza ordering system would produce events
  and publish to a message broker service (AWS Kinesis/Apache Kafka).

- Streaming Job
  A spark streaming job would keep on consuming streaming data from the message broker.
  As per the analytics requirement we could also partition the data according to store_id, date, hour
  and save this analytical data on storage block (HDFS or AWS S3).

- Batch jobs
  All the stores POS files are sent to a SFTP server. An Apache Airflow/cron/File Watcher job
  could be scheduled to run after 15 minutes to copy this data to HDFS/S3 buckets. A spark batch
  job could be spun after every 15 minutes to transform this data and write this data back to storage
  with partitons (as per requirements). As this prepared analytical data would be read heavy, we could save this
  data in parquet/orc file format for higher read throughput.

- Analytical layer
  Apache Hive/AWS Redshift could be utilized to provide more insights on top of the analytical
  data both batch and streaming jobs have generated.


** Note
- Attached is the Pizza_house_system_design.pdf which contains the high level design.
- Also the file format mentioned in the .pdf design diagram should be parquet instead of avro.
  
